# -*- coding: utf-8 -*-
"""Assignment2phase2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FCTmFLty1om21ddCPLkp-8GWDO0wFa2j
"""

print("Question 2")

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

! pip install -q kaggle

! mkdir ~/.kaggle

!cp /content/drive/MyDrive/timepass/Kaggle/kaggle.json ~/.kaggle/kaggle.json

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets list

!kaggle datasets download -d robikscube/driving-video-with-object-tracking

import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

!pip install opencv-python-headless tensorflow

import os
import cv2
import numpy as np
import tensorflow as tf
import zipfile

# Mount Google Drive (if your videos are stored in Google Drive)
from google.colab import drive
drive.mount('/content/drive')

# Path to the pre-trained object detection model
model_path = 'path_to_model_directory'  # Replace with your model path
output_path = '/content/output_videos'   # Output directory for processed videos
zip_file_path = '/content/driving-video-with-object-tracking.zip'  # Path to the ZIP file
unzip_dir = '/content/unzipped_videos'   # Directory to unzip the videos

# Create the output directory if it doesn't exist
os.makedirs(output_path, exist_ok=True)

# Function to preprocess a frame (replace with your preprocessing logic)
def preprocess(frame):
    # Your preprocessing code here
    return frame

# Function to draw bounding boxes (replace with your drawing logic)
def draw_boxes(frame, detections):
    # Your drawing code here
    return frame

# Extract the ZIP file containing videos
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(unzip_dir)

# List all video files in the unzipped directory
video_files = [f for f in os.listdir(unzip_dir) if f.endswith('.mov')]

!pip install opencv-python-headless tensorflow pandas pyarrow

import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from object_detection.utils import config_util
from object_detection.builders import model_builder
from object_detection.utils import visualization_utils as viz_utils
from object_detection.utils import label_map_util

# Define paths to your data and model
video_dir = '/content/unzipped_videos/bdd100k_videos_train_00/bdd100k/videos/train'
label_file_csv = '/content/unzipped_videos/mot_labels.csv'
label_file_parquet = '/content/unzipped_videos/mot_labels.parquet'
model_dir = '/content/unzipped_videos'

# Create a label map and load labels (if available)
label_map_path = 'path_to_label_map.pbtxt'  # You'll need to create this file
label_map = label_map_util.load_labelmap(label_map_path)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=90)
category_index = label_map_util.create_category_index(categories)

# Load your video data
video_files = [f for f in os.listdir(video_dir) if f.endswith('.mov')]

# Load your annotations (labels)
annotations_csv = pd.read_csv(label_file_csv)
annotations_parquet = pd.read_parquet(label_file_parquet)

# Load a pre-trained object detection model
def load_model(model_dir):
    # Load pipeline config and build detection model
    pipeline_config = os.path.join(model_dir, 'pipeline.config')
    configs = config_util.get_configs_from_pipeline_file(pipeline_config)
    model_config = configs['model']
    detection_model = model_builder.build(model_config=model_config, is_training=False)

    # Restore checkpoint
    ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
    ckpt.restore(os.path.join(model_dir, 'checkpoint', 'ckpt-0')).expect_partial()

    return detection_model

detection_model = load_model(model_dir)

# Function to perform object detection on a frame
def detect_objects(frame):
    input_tensor = tf.convert_to_tensor(np.expand_dims(frame, 0))
    detections = detection_model(input_tensor)
    return detections

# Function to draw bounding boxes on a frame
def draw_boxes_on_frame(frame, detections):
    viz_utils.visualize_boxes_and_labels_on_image_array(
        frame,
        detections['detection_boxes'][0].numpy(),
        detections['detection_classes'][0].numpy().astype(np.int32),
        detections['detection_scores'][0].numpy(),
        category_index,
        use_normalized_coordinates=True,
        max_boxes_to_draw=200,  # Adjust as needed
        min_score_thresh=0.30,  # Adjust as needed
        agnostic_mode=False)
    return frame

# Function to process and save a video
def process_and_save_video(input_video_path, output_video_path):
    cap = cv2.VideoCapture(input_video_path)
    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))
    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (frame_width, frame_height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Perform object detection on the frame
        detections = detect_objects(frame)

        # Draw bounding boxes on the frame
        frame_with_boxes = draw_boxes_on_frame(frame, detections)

        # Write the frame with bounding boxes to the output video
        out.write(frame_with_boxes)

    cap.release()
    out.release()

# Process and save each video
for video_file in video_files:
    input_video_path = os.path.join(video_dir, video_file)
    output_video_path = os.path.join(output_path, video_file)
    process_and_save_video(input_video_path, output_video_path)

!pip install opencv-python-headless numpy tensorflow

import os

output_folder = "/content/output_videos"
os.makedirs(output_folder, exist_ok=True)

!pip install opencv-python-headless numpy

import cv2
import numpy as np

# Load YOLOv4 model
net = cv2.dnn.readNet("yolov4.weights", "yolov4.cfg")

# Load class names
classes = []
with open("coco.names", "r") as f:
    classes = f.read().strip().split("\n")

# Input and output paths
input_folder = "/content/unzipped_videos/bdd100k_videos_train_00/bdd100k/videos/train/"
output_folder = "/content/output_videos"

# Process each video
for video_file in os.listdir(input_folder):
    input_video_path = os.path.join(input_folder, video_file)
    output_video_path = os.path.join(output_folder, f"processed_{video_file}")

    cap = cv2.VideoCapture(input_video_path)

    # Get video properties
    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))
    fps = int(cap.get(5))

    # Define the codec and create a VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)
        net.setInput(blob)

        # Perform object detection
        layer_names = net.getUnconnectedOutLayersNames()
        detections = net.forward(layer_names)

        # Process detections and draw bounding boxes (similar to previous example)
        # ...

        # Write the frame with bounding boxes to the output video
        out.write(frame)

    # Release video objects
    cap.release()
    out.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

!pip install ultralytics -q

import zipfile

# Specify the path to the ZIP file you want to unzip
zip_file_path = '/content/driving-video-with-object-tracking.zip'

# Specify the directory where you want to extract the contents
extracted_dir_path = '/content/'

# Use the zipfile module to extract the contents
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir_path)

# Commented out IPython magic to ensure Python compatibility.
# Install Darknet (adjust based on your YOLO version)
!git clone https://github.com/AlexeyAB/darknet.git
# %cd darknet
!make

# Install OpenCV
!pip install opencv-python

import os

video_directory = "/content/bdd100k_videos_train_00/bdd100k/videos/train"
video_files = os.listdir(video_directory)
selected_videos = video_files[:20]  # Adjust the slice to select the first 20 videos

# Create a list of full paths to the selected videos
video_paths = [os.path.join(video_directory, video) for video in selected_videos]

import os
import cv2
import subprocess

# Define the path to your YOLO model
model_path = "yolov8m.pt"

# Define the directory containing the videos
video_directory = "/content/bdd100k_videos_train_00/bdd100k/videos/train"

# Define the output directory for processed videos
output_directory = "/content/output_videos"
os.makedirs(output_directory, exist_ok=True)

# List all video files in the directory
video_files = os.listdir(video_directory)

# Select the first 20 videos (adjust as needed)
selected_videos = video_files[:20]

# Loop through the selected videos
for video_file in selected_videos:
    video_path = os.path.join(video_directory, video_file)

    # Define the output video file path
    output_path = os.path.join(output_directory, os.path.basename(video_file))

    # Define the YOLO command
    yolo_command = f"!yolo detect predict model={model_path} source={video_path} output={output_path}"

    # Run the YOLO command using subprocess
    subprocess.run(yolo_command, shell=True)

    # Create a command to add libx264 codec using FFmpeg
    ffmpeg_command = f"!ffmpeg -i {output_path} -vcodec libx264 {os.path.join(output_directory, 'vid_' + os.path.basename(output_path))}"

    # Run the FFmpeg command using subprocess
    subprocess.run(ffmpeg_command, shell=True)


# Release all resources
cv2.destroyAllWindows()

!yolo detect predict model=yolov8m.pt source="/content/drive/MyDrive/timepass/video"

!pip install ultralytics -q

!yolo detect predict model=yolov8m.pt source="/content/drive/MyDrive/timepass/video"

!ffmpeg -i {"/content/darknet/runs/detect/predict/1.avi"} -vcodec libx264 {"video1.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/10.avi"} -vcodec libx264 {"video10.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/11.avi"} -vcodec libx264 {"video11.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/12.avi"} -vcodec libx264 {"video12.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/13.avi"} -vcodec libx264 {"video13.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/14.avi"} -vcodec libx264 {"video14.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/15.avi"} -vcodec libx264 {"video15.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/16.avi"} -vcodec libx264 {"video16.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/17.avi"} -vcodec libx264 {"video17.avi"}
!ffmpeg -i {"/content/darknet/runs/detect/predict/18.avi"} -vcodec libx264 {"video18.avi"}

!ffmpeg -i {"/content/darknet/runs/detect/predict/1.avi"} -vcodec libx264 {"video1.avi"}

!ffmpeg -i {"/content/darknet/runs/detect/predict/1.avi"} -vcodec libx264 {"/content/drive/MyDrive/timepass/Output/video1.avi"}

